---
layout: distill
title: Best-of-N Sampling with Mis-specified Dense Reward Models
description: 
tags: llm
giscus_comments: true
date: 2025-04-16
featured: true

authors:
  - name: Lin Gui
    url: "https://gl-ybnbxb.github.io/"
    affiliations:
      name: Uchicago

# bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Preliminaries
  - name: Preference-based Tasks
    # if a section has subsections, you can add them as follows:
    # subsections:
      - name: Win rate is a better metric than vanilla reward
      - name: BoN sampling is mostly affected by mis-specification of high-rewards
  - name: Binary Tasks (A Mixture Model)
  - name: Citation
  # - name: Code Blocks
  # - name: Interactive Plots
  # - name: Layouts
  # - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


**Main takeaways**
1. The performance of BoN sampling is primarily affected by the accuracy of high rewards by a reward model
2. Small $n$ won't hurt BoN sampling's performance
3. For binary tasks, BoN sampling remains effective as long as the reward model can differentiate between correct and incorrect modes a little bit


## Preliminaries

**Notations**
- $\pi_0$: a large language model
- $x$: a prompt
- $y$: a response
- $Y$: a random variable (response)
- $D$: a prompt set
- $r^*$: true reward
- $r$: a proxy reward model

- Best-of-N sampling
    - Sample $y_1,\dots,y_n$ independently from $\pi_0(\cdot\mid x)$
    - Select $y=y_i$, such that $r(x,y_i)=\max_{i\in[n]} r(x,y_i)$

## Preference-based Tasks

### Win rate is a better metric than vanilla reward

Human preference alignment for large language models (LLMs) aims to bias model outputs toward desirable attributes such as helpfulness and factuality. The aligned model should also maintain its pre-trained capabilities by staying close to the original model. To guide this alignment, a reward function is used—it assigns a value $r(x,y)$ that measures the quality of a response $y$ for a prompt $x$. 

Let's first assume we have access to the true reward $r^*$. Here, "true" means the reward function $r^*$ can correctly rank all possible responses. Methods like [RLHF](https://arxiv.org/abs/2203.02155) and [DPO](https://arxiv.org/abs/2305.18290) optimize the expected reward while adding a KL divergence penalty to control model drift. That is

$$
\pi=\argmax_\pi\mathbb{E}_{x\sim D, y\sim\pi(\cdot\mid x)}[r^*(x,y)]-\beta\mathrm{KL}(\pi\|\pi_0)
$$

However, rewards often arise from relative preferences. For example, the Bradley-Terry model tries to capture the rank of responses reflecting human preference. In this sense, the ranking of responses matters more than their absolute rewards. Additionally, maximizing expected reward can be arbitrary since any monotonic transformation preserves rankings while changing the optimization landscape. Therefore, win rate serves as a better summary statistic:

$$
\mathbb{P}(r^*(x,Y)\ge r^*(x,Y_0)),~~Y\sim\pi(\cdot\mid x),~Y_0\sim\pi_0(\cdot\mid x).
$$

Since win rate is invariant to monotonic transformations of $r^*$, it provides a robaust evaluation metric. This leads to the following mathematical formulation of the alignment problem:

$$
\pi=\argmax_\pi\mathbb{E}_{x\sim D}\mathbb{P}_{y\sim\pi(\cdot\mid x),y_0\sim\pi_0(\cdot\mid x)}\left(r^*(x,y)\ge r^*(x,y_0)\right)]-\beta\mathrm{KL}(\pi\|\pi_0)~~~(1)
$$

For a prompt $x$, denote $Q_x$ the cumulative distribution function of $r^*(x,Y_0)$ where $Y_0\sim\pi_0(\cdot\mid x)$. For a response $y\sim\pi_0(\cdot\mid x)$, its rank among all responses is $Q_x(r(x,y))=\mathbb{P}(r^*(x,Y_0)\le r(x,y))$. For example, if $Q_x(r(x,y))=0.6$, then there are 40% responses better than this $y$. The win rate can be further rewritten as 

$$
\Pr(r^*(x,Y)\ge r^*(x,Y_0))=\mathbb{E}_{Y\sim\pi(\cdot\mid x)}\left[Q_x(r(x,Y))\right].
$$

Then, (1) becomes

$$
\pi=\argmax_\pi\mathbb{E}_{x\sim D, y\sim\pi(\cdot\mid x)}[Q_x\left(r^*(x,y)\right)]-\beta\mathrm{KL}(\pi\|\pi_0),
$$

which can also be seen as original RLHF/DPO training objective but with prompt-specific normalized rewards.

### BoN sampling is mostly affected by mis-specification of high-rewards

According to [BoNBoN paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/056521a35eacd9d2127b66a7d3c499c5-Paper-Conference.pdf), the underlying distribution of BoN sampling is 

$$
\pi_r^{(n)}(y\mid x)nQ_x(r(x,y))^{n-1}\pi_0(y\mid x),
$$

and it is essentially the optimal policy to balance win rate and KL divergence. However, in practice, its actual win rate cannot be attained since only a proxy reward $r$ (instead of true reward $r^*$) is accessible. 

<aside>

**Theorem 1**: If the rewards have a continuous distribution, the win rate can be expressed in closed form as 

$$
\int_0^1ng(u)u^{n-1}\mathrm{d}u,
$$

where $g$ is a function such that

$$
g(U)\overset{d}{=}U\sim U(0,1).
$$

</aside>

Note that the KL divergence stays the same no matter what reward model is using. Given an mis-specified reward model unable to identify correct ranks of responses, the win rate vs KL divergence curve is no longer Pareto frontier. To get a better understanding of how reward model affects the performance of BoN sampling, we simulate the following setting: the true reward model is CDF of the uniform distribution $U(0,1)$, that is, $r^*(y)=y$. We consider several reward models with different levels of ranking mis-specification:

- $r_1(y)=1-y$: the ranking is reversed;
- $r_2(y)=y\mathbb{1}_{y\le0.5}+(1.5-y)\mathbb{1}_{y>0.5}$: top 50\% of the responses are ranked correctly while worst 50\% are reversed;
- $r_3(y)=y\mathbb{1}_{y\le0.75}+(1.75-y)\mathbb{1}_{y>0.25}$: top 25\% of the responses are ranked correctly while worst 75\% are reversed.
- $r_4(y)=(0.5-y)\mathbb{1}_{y\le0.5}+y\mathbb{1}_{y>0.5}$: worst 50\% of the responses are reversed while top 50\% are ranked correctly.
- $r_5(y)=(0.9-y)\mathbb{1}_{y\le0.9}+y\mathbb{1}_{y>0.9}$: worst 90\% of the responses are reversed while top 10\% are ranked correctly.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/Reward_misspecification.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Figure 1: Actual win rate of BoN sampling versus its KL divergence with mis-specified rewards.
</div>

Figure 1 shows the actual win rate of BoN sampling applying each reward model, plotted against KL divergence. It’s obvious that mis-specification of a reward function in the high-reward region has a much larger impact on the actual win rate than that in low-reward region. This is intuitive—BoN assigns a polynomial of rewards as weights to high-reward responses, making it especially sensitive to inaccuracies in that region. 

Interestingly, increasing $n$ does not degrade BoN sampling’s performance even when the rewards of the worst 90% responses are reversed. Intuitively, in this case, since the sampling process still selects one response with a true high reward as $n$ grows to infinity, allowing the win rate to eventually approach its optimum. In reverse, if the reward mis-specification occurs in the high-reward region, the actual win rate initially improves but then drops sharply once $n$ becomes moderately large.

A practical compromise is to choose a small $n$ for BoN sampling. If we trust the proxy reward model to roughly preserve the ranking of responses, then there exists an optimal $n$ that maximizes a lower bound of the actual win rate.

<aside>

**Theorem 2**: Suppose the proxy reward function $r$ can separate top $(1-c)$% responses:

$$
\mathbb{1}_{Q_x(r(x,y))\ge c}=\mathbb{1}_{Q^*_x(r^*(x,y))\ge c},
$$

where $Q_x$ and $Q^*_x$ are CDFs of $r(x,Y_0)$ and $r(x,Y)$ with $Y_0\sim\pi_0(\cdot\mid x)$ and $Y\sim\pi(\cdot\mid x)$. Then the win rate $p$ of BoN sampling has the lower bound

$$
p\ge\frac{1}{n+1}+c-c^n.
$$

</aside>

The optimal $n$s which maximize this lower bound are shown in Table 1.

| $c$ |             0.5 |            0.6 |            0.7 |            0.8 |            0.9 |
| --- | --- | --- | --- | --- | --- |
| Optimal $n$ |               4 |              7 |             11 |             21 |             55 |
| Lower bound of win rate |        63.75% |        69.70% |         76.36% |         83.62% |         91.48% |


*Remark 1: if $c$ can be as high as 90%, i.e., the proxy reward model can identify top 10% responses (although unable to rank within them), the lower bound of win rate will eventually converge to 0.9 as $n\to\infty$.*

*Remark 2: in practice, the threshold for each prompt $x$ is different, i.e. the threshold should be $c_x$ for a fixed prompt $x$. Then the lower bound of the expected win rate is $\mathbb{E}_{x\sim D}\left[\frac{1}{n+1}+c_x-c_x^n\right]$.*

## Binary Tasks: A Mixture Model

For those tasks with objective answers, such as math reasoning or code generation, we only care about the correctness of responses. Given a prompt $x$, let  $p_x$ represent the underlying accuracy of the language model $\pi_0$. Then for a randomly generated response $Y\sim\pi_0(\cdot\mid x)$, its correctness follow a binomial distribution:

$$
\mathbb{1}_{(Y~\mathrm{is~correct})}\sim\mathrm{Binomial}(p_x).
$$

Then the distribution of Pass@$n$ is

$$
\mathbb{1}_{(\mathrm{at~least~one}~Y_i~\mathrm{is~ correct})}\sim\mathrm{Binomial}(1-(1-p_x)^n).
$$

For a dense reward model, say ORM for math reasoning, the distribution of $r(x,Y_0)$ with $Y_0\sim\pi_0(\cdot\mid x)$ should be a mixture model:
$$
p_x f_1(r)+(1-p_x) f_0(r),
$$
where $f_1$ and $f_0$ are densities of $r(x,Y_0)\mid \mathbb{1}_{\left(Y_0\mathrm{~is~correct}\right)}=1$ and $r(x,Y_0)\mid \mathbb{1}_{\left(Y_0~\mathrm{is~incorrect}\right)}=0$. Further denote $F_1$ and $F_0$ corresponding CDFs. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/density.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Figure 2: the density of a mixture model.
</div>

Then the accuracy of BoN sampling is 
$$
\int n p_x f_1(r)\left\{p_x F_1(r)+(1-p_x)F_0(r)\right\}^{n-1}\mathrm{d}r.~~~~~~~(2)
$$

In the extreme case where the reward model fails to distinguish between correct and incorrect responses, i.e., $F_0(r)=F_1(r)$. Then (2) simplifies to $p_x$. In this scenario, BoN sampling offers no advantage: its accuracy matches that of a single random sample from the base model.

However, if the reward model provides even slight separation between correct and incorrect responses, BoN sampling becomes effective. As $n$ increases, the accuracy of BoN sampling eventually approaches 1. In other words, a large $n$ doesn’t hurt the  performance of BoN.

<aside>

**Theorem 3**: Let $c_0=\inf\{r:F_0(r)=1\}$. Suppose $F_1(c_0)<1$. Then (2) has a lower bound

$$
p_x^nF_1(c_0)^n+1-\left\{p_xF_1(c_0)+1-p_x\right\}^{n-1}.
$$

Moreover, this lower bound converges to 1 as $n\to\infty$. Hence, the accuracy of BoN sampling also converges to 1.

</aside>

## Citation

```
@misc{guibon2025,
  title={Best-of-N Sampling with Mis-specified Reward Models},
  author={Lin Gui},
  howpublished={\url{https://}},
  year={2025}
}
```

<!-- ## Interactive Plots

You can add interative plots using plotly + iframes :framed_picture:

<div class="l-page">
  <iframe src="{{ '/assets/plotly/demo.html' | relative_url }}" frameborder='0' scrolling='no' height="500px" width="100%" style="border: 1px dashed grey;"></iframe>
</div>

The plot must be generated separately and saved into an HTML file.
To generate the plot that you see above, you can use the following code snippet:

{% highlight python %}
import pandas as pd
import plotly.express as px
df = pd.read_csv(
'https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv'
)
fig = px.density_mapbox(
df,
lat='Latitude',
lon='Longitude',
z='Magnitude',
radius=10,
center=dict(lat=0, lon=180),
zoom=0,
mapbox_style="stamen-terrain",
)
fig.show()
fig.write_html('assets/plotly/demo.html')
{% endhighlight %}
 -->

<!-- ## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div> -->


