<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gl-ybnbxb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gl-ybnbxb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-16T18:28:14+00:00</updated><id>https://gl-ybnbxb.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html"></title><link href="https://gl-ybnbxb.github.io/blog/2025/2025-4-16-bon/" rel="alternate" type="text/html" title=""/><published>2025-04-16T18:28:14+00:00</published><updated>2025-04-16T18:28:14+00:00</updated><id>https://gl-ybnbxb.github.io/blog/2025/2025-4-16-bon</id><content type="html" xml:base="https://gl-ybnbxb.github.io/blog/2025/2025-4-16-bon/"><![CDATA[<p><strong>Main takeaways</strong></p> <ol> <li>The performance of BoN sampling is primarily affected by the accuracy of high rewards by a reward model</li> <li>Small $n$ won’t hurt BoN sampling’s performance</li> <li>For binary tasks, BoN sampling remains effective as long as the reward model can differentiate between correct and incorrect modes a little bit</li> </ol> <h2 id="preliminaries">Preliminaries</h2> <p><strong>Notations</strong></p> <ul> <li>$\pi_0$: a large language model</li> <li>$x$: a prompt</li> <li>$y$: a response</li> <li>$Y$: a random variable (response)</li> <li>$D$: a prompt set</li> <li>$r^*$: true reward</li> <li> <p>$r$: a proxy reward model</p> </li> <li>Best-of-N sampling <ul> <li>Sample $y_1,\dots,y_n$ independently from $\pi_0(\cdot\mid x)$</li> <li>Select $y=y_i$, such that $r(x,y_i)=\max_{i\in[n]} r(x,y_i)$</li> </ul> </li> </ul> <h2 id="preference-based-tasks">Preference-based Tasks</h2> <h3 id="win-rate-is-a-better-metric-than-vanilla-reward">Win rate is a better metric than vanilla reward</h3> <p>Human preference alignment for large language models (LLMs) aims to bias model outputs toward desirable attributes such as helpfulness and factuality. The aligned model should also maintain its pre-trained capabilities by staying close to the original model. To guide this alignment, a reward function is used—it assigns a value $r(x,y)$ that measures the quality of a response $y$ for a prompt $x$.</p> <p>Let’s first assume we have access to the true reward $r^<em>$. Here, “true” means the reward function $r^</em>$ can correctly rank all possible responses. Methods like <a href="https://arxiv.org/abs/2203.02155">RLHF</a> and <a href="https://arxiv.org/abs/2305.18290">DPO</a> optimize the expected reward while adding a KL divergence penalty to control model drift. That is</p> \[\pi=\argmax_\pi\mathbb{E}_{x\sim D, y\sim\pi(\cdot\mid x)}[r^*(x,y)]-\beta\mathrm{KL}(\pi\|\pi_0)\] <p>However, rewards often arise from relative preferences. For example, the Bradley-Terry model tries to capture the rank of responses reflecting human preference. In this sense, the ranking of responses matters more than their absolute rewards. Additionally, maximizing expected reward can be arbitrary since any monotonic transformation preserves rankings while changing the optimization landscape. Therefore, win rate serves as a better summary statistic:</p> \[\mathbb{P}(r^*(x,Y)\ge r^*(x,Y_0)),~~Y\sim\pi(\cdot\mid x),~Y_0\sim\pi_0(\cdot\mid x).\] <p>Since win rate is invariant to monotonic transformations of $r^*$, it provides a robaust evaluation metric. This leads to the following mathematical formulation of the alignment problem:</p> \[\pi=\argmax_\pi\mathbb{E}_{x\sim D}\mathbb{P}_{y\sim\pi(\cdot\mid x),y_0\sim\pi_0(\cdot\mid x)}\left(r^*(x,y)\ge r^*(x,y_0)\right)]-\beta\mathrm{KL}(\pi\|\pi_0)~~~(1)\] <p>For a prompt $x$, denote $Q_x$ the cumulative distribution function of $r^<em>(x,Y_0)$ where $Y_0\sim\pi_0(\cdot\mid x)$. For a response $y\sim\pi_0(\cdot\mid x)$, its rank among all responses is $Q_x(r(x,y))=\mathbb{P}(r^</em>(x,Y_0)\le r(x,y))$. For example, if $Q_x(r(x,y))=0.6$, then there are 40% responses better than this $y$. The win rate can be further rewritten as</p> \[\Pr(r^*(x,Y)\ge r^*(x,Y_0))=\mathbb{E}_{Y\sim\pi(\cdot\mid x)}\left[Q_x(r(x,Y))\right].\] <p>Then, (1) becomes</p> \[\pi=\argmax_\pi\mathbb{E}_{x\sim D, y\sim\pi(\cdot\mid x)}[Q_x\left(r^*(x,y)\right)]-\beta\mathrm{KL}(\pi\|\pi_0),\] <p>which can also be seen as original RLHF/DPO training objective but with prompt-specific normalized rewards.</p> <h3 id="bon-sampling-is-mostly-affected-by-mis-specification-of-high-rewards">BoN sampling is mostly affected by mis-specification of high-rewards</h3> <p>According to <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/056521a35eacd9d2127b66a7d3c499c5-Paper-Conference.pdf">BoNBoN paper</a>, the underlying distribution of BoN sampling is</p> \[\pi_r^{(n)}(y\mid x)nQ_x(r(x,y))^{n-1}\pi_0(y\mid x),\] <p>and it is essentially the optimal policy to balance win rate and KL divergence. However, in practice, its actual win rate cannot be attained since only a proxy reward $r$ (instead of true reward $r^*$) is accessible.</p> <aside> **Theorem 1**: If the rewards have a continuous distribution, the win rate can be expressed in closed form as $$ \int_0^1ng(u)u^{n-1}\mathrm{d}u, $$ where $g$ is a function such that $$ g(U)\overset{d}{=}U\sim U(0,1). $$ </aside> <p>Note that the KL divergence stays the same no matter what reward model is using. Given an mis-specified reward model unable to identify correct ranks of responses, the win rate vs KL divergence curve is no longer Pareto frontier. To get a better understanding of how reward model affects the performance of BoN sampling, we simulate the following setting: the true reward model is CDF of the uniform distribution $U(0,1)$, that is, $r^*(y)=y$. We consider several reward models with different levels of ranking mis-specification:</p> <ul> <li>$r_1(y)=1-y$: the ranking is reversed;</li> <li>$r_2(y)=y\mathbb{1}<em>{y\le0.5}+(1.5-y)\mathbb{1}</em>{y&gt;0.5}$: top 50\% of the responses are ranked correctly while worst 50\% are reversed;</li> <li>$r_3(y)=y\mathbb{1}<em>{y\le0.75}+(1.75-y)\mathbb{1}</em>{y&gt;0.25}$: top 25\% of the responses are ranked correctly while worst 75\% are reversed.</li> <li>$r_4(y)=(0.5-y)\mathbb{1}<em>{y\le0.5}+y\mathbb{1}</em>{y&gt;0.5}$: worst 50\% of the responses are reversed while top 50\% are ranked correctly.</li> <li>$r_5(y)=(0.9-y)\mathbb{1}<em>{y\le0.9}+y\mathbb{1}</em>{y&gt;0.9}$: worst 90\% of the responses are reversed while top 10\% are ranked correctly.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Reward_misspecification.png" sizes="95vw"/> <img src="/assets/img/Reward_misspecification.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Actual win rate of BoN sampling versus its KL divergence with mis-specified rewards. </div> <p>Figure 1 shows the actual win rate of BoN sampling applying each reward model, plotted against KL divergence. It’s obvious that mis-specification of a reward function in the high-reward region has a much larger impact on the actual win rate than that in low-reward region. This is intuitive—BoN assigns a polynomial of rewards as weights to high-reward responses, making it especially sensitive to inaccuracies in that region.</p> <p>Interestingly, increasing $n$ does not degrade BoN sampling’s performance even when the rewards of the worst 90% responses are reversed. Intuitively, in this case, since the sampling process still selects one response with a true high reward as $n$ grows to infinity, allowing the win rate to eventually approach its optimum. In reverse, if the reward mis-specification occurs in the high-reward region, the actual win rate initially improves but then drops sharply once $n$ becomes moderately large.</p> <p>A practical compromise is to choose a small $n$ for BoN sampling. If we trust the proxy reward model to roughly preserve the ranking of responses, then there exists an optimal $n$ that maximizes a lower bound of the actual win rate.</p> <aside> **Theorem 2**: Suppose the proxy reward function $r$ can separate top $(1-c)$% responses: $$ \mathbb{1}_{Q_x(r(x,y))\ge c}=\mathbb{1}_{Q^*_x(r^*(x,y))\ge c}, $$ where $Q_x$ and $Q^*_x$ are CDFs of $r(x,Y_0)$ and $r(x,Y)$ with $Y_0\sim\pi_0(\cdot\mid x)$ and $Y\sim\pi(\cdot\mid x)$. Then the win rate $p$ of BoN sampling has the lower bound $$ p\ge\frac{1}{n+1}+c-c^n. $$ </aside> <p>The optimal $n$s which maximize this lower bound are shown in Table 1.</p> <table> <thead> <tr> <th>$c$</th> <th>0.5</th> <th>0.6</th> <th>0.7</th> <th>0.8</th> <th>0.9</th> </tr> </thead> <tbody> <tr> <td>Optimal $n$</td> <td>4</td> <td>7</td> <td>11</td> <td>21</td> <td>55</td> </tr> <tr> <td>Lower bound of win rate</td> <td>63.75%</td> <td>69.70%</td> <td>76.36%</td> <td>83.62%</td> <td>91.48%</td> </tr> </tbody> </table> <p><em>Remark 1: if $c$ can be as high as 90%, i.e., the proxy reward model can identify top 10% responses (although unable to rank within them), the lower bound of win rate will eventually converge to 0.9 as $n\to\infty$.</em></p> <p><em>Remark 2: in practice, the threshold for each prompt $x$ is different, i.e. the threshold should be $c_x$ for a fixed prompt $x$. Then the lower bound of the expected win rate is $\mathbb{E}_{x\sim D}\left[\frac{1}{n+1}+c_x-c_x^n\right]$.</em></p> <h2 id="binary-tasks-a-mixture-model">Binary Tasks: A Mixture Model</h2> <p>For those tasks with objective answers, such as math reasoning or code generation, we only care about the correctness of responses. Given a prompt $x$, let $p_x$ represent the underlying accuracy of the language model $\pi_0$. Then for a randomly generated response $Y\sim\pi_0(\cdot\mid x)$, its correctness follow a binomial distribution:</p> \[\mathbb{1}_{(Y~\mathrm{is~correct})}\sim\mathrm{Binomial}(p_x).\] <p>Then the distribution of Pass@$n$ is</p> \[\mathbb{1}_{(\mathrm{at~least~one}~Y_i~\mathrm{is~ correct})}\sim\mathrm{Binomial}(1-(1-p_x)^n).\] <p>For a dense reward model, say ORM for math reasoning, the distribution of $r(x,Y_0)$ with $Y_0\sim\pi_0(\cdot\mid x)$ should be a mixture model: \(p_x f_1(r)+(1-p_x) f_0(r),\) where $f_1$ and $f_0$ are densities of $r(x,Y_0)\mid \mathbb{1}<em>{\left(Y_0\mathrm{~is~correct}\right)}=1$ and $r(x,Y_0)\mid \mathbb{1}</em>{\left(Y_0~\mathrm{is~incorrect}\right)}=0$. Further denote $F_1$ and $F_0$ corresponding CDFs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/density.png" sizes="95vw"/> <img src="/assets/img/density.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: the density of a mixture model. </div> <p>Then the accuracy of BoN sampling is \(\int n p_x f_1(r)\left\{p_x F_1(r)+(1-p_x)F_0(r)\right\}^{n-1}\mathrm{d}r.~~~~~~~(2)\)</p> <p>In the extreme case where the reward model fails to distinguish between correct and incorrect responses, i.e., $F_0(r)=F_1(r)$. Then (2) simplifies to $p_x$. In this scenario, BoN sampling offers no advantage: its accuracy matches that of a single random sample from the base model.</p> <p>However, if the reward model provides even slight separation between correct and incorrect responses, BoN sampling becomes effective. As $n$ increases, the accuracy of BoN sampling eventually approaches 1. In other words, a large $n$ doesn’t hurt the performance of BoN.</p> <aside> **Theorem 3**: Let $c_0=\inf\{r:F_0(r)=1\}$. Suppose $F_1(c_0)&lt;1$. Then (2) has a lower bound $$ p_x^nF_1(c_0)^n+1-\left\{p_xF_1(c_0)+1-p_x\right\}^{n-1}. $$ Moreover, this lower bound converges to 1 as $n\to\infty$. Hence, the accuracy of BoN sampling also converges to 1. </aside> <h2 id="citation">Citation</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{guibon2025,
  title={Best-of-N Sampling with Mis-specified Reward Models},
  author={Lin Gui},
  howpublished={\url{https://}},
  year={2025}
}
</code></pre></div></div> ]]></content><author><name></name></author></entry><entry><title type="html">Your connected workspace for wiki, docs &amp;amp; projects | Notion</title><link href="https://gl-ybnbxb.github.io/blog/2025/your-connected-workspace-for-wiki-docs-projects-notion/" rel="alternate" type="text/html" title="Your connected workspace for wiki, docs &amp;amp; projects | Notion"/><published>2025-04-16T00:00:00+00:00</published><updated>2025-04-16T00:00:00+00:00</updated><id>https://gl-ybnbxb.github.io/blog/2025/your-connected-workspace-for-wiki-docs--projects--notion</id><content type="html" xml:base="https://gl-ybnbxb.github.io/blog/2025/your-connected-workspace-for-wiki-docs-projects-notion/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team]]></summary></entry></feed>